{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krahul2024/machine-learning/blob/main/projects/next_char.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Working on next-character prediction model"
      ],
      "metadata": {
        "id": "5vs7_WizNqOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import requests\n",
        "\n",
        "file_url = 'https://raw.githubusercontent.com/krahul2024/machine-learning/main/projects/Text-Datasets/paul_graham_essays.txt'\n",
        "data = requests.get(file_url).text\n",
        "print(len(data))"
      ],
      "metadata": {
        "id": "c2hRHa-kNzTF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc98903a-2b60-404b-ce50-c6579a89038e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3023219\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Data Pre-Processing\n",
        "\n",
        "def process_data(content, input_size = 30):\n",
        "  import numpy as np\n",
        "  vocab = sorted(list(set(content)))\n",
        "  vocab_size, total_chars = len(vocab), len(content)\n",
        "  print(f\"Total unique characters : {vocab_size}\")\n",
        "\n",
        "  char_index = dict((c, i) for i, c in enumerate(vocab)) # map the characters against their respective indexes\n",
        "  steps = 3 # the characters we would skip to construct next input sequence\n",
        "  x , y = [], []\n",
        "\n",
        "  # create the dataset\n",
        "  for i in range(0, total_chars-input_size, steps):\n",
        "      x.append(content[i:i+input_size])\n",
        "      y.append(content[i+input_size])\n",
        "\n",
        "  # print first 5 sample input and outputs from our dataset\n",
        "  for i in range(5):\n",
        "      print(f\"{x[i]} -> {y[i]}\")\n",
        "\n",
        "  print(f\"input-size : {len(x)}, output-size : {len(y)}\")\n",
        "\n",
        "  # convert the input and output to numpy array format, also perform the encoding\n",
        "  X = np.zeros(\n",
        "      (\n",
        "          len(x),\n",
        "          input_size,\n",
        "          len(vocab)\n",
        "      ),\n",
        "      dtype =bool\n",
        "  )\n",
        "\n",
        "  Y = np.zeros(\n",
        "      (\n",
        "          len(x),\n",
        "          vocab_size\n",
        "      )\n",
        "  )\n",
        "\n",
        "  print(f\"Dims of input data : {X.shape} \\n Dims of output data : {Y.shape}\")\n",
        "\n",
        "  # Assign the truth values where we find a character and false values to rest of the array, do the same thing for input and output data\n",
        "  for i, seq in enumerate(x):\n",
        "    for j, char in enumerate(seq):\n",
        "      X[i, j, char_index[char]] = 1\n",
        "      Y[i, char_index[y[i]]] = 1\n",
        "\n",
        "  # split the large dataset into multiple parts taking into consideration the resource availability\n",
        "  train_X = np.array_split(X, 10)\n",
        "  train_Y = np.array_split(Y, 10)\n",
        "\n",
        "  return [vocab, char_index, train_X, train_Y]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "LpDGUkHLRf7o"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model Building and Training function\n",
        "\n",
        "# function to create and compile the model\n",
        "def create_model(input_size, vocab_size,):\n",
        "  import tensorflow as tf\n",
        "  model_ = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.LSTM(128, input_shape = (input_size, vocab_size)),\n",
        "      tf.keras.layers.Dense(vocab_size, activation = 'softmax')\n",
        "  ])\n",
        "\n",
        "  model_.compile(\n",
        "      optimizer = 'adam',\n",
        "      loss = 'categorical_crossentropy',\n",
        "      metrics = ['accuracy']\n",
        "  )\n",
        "  print(model_.summary())\n",
        "  return model_\n",
        "\n",
        "# function to train the model\n",
        "def train_model(model_, input_data, output_data, batch_size = 128, validation_split = 0.05, verbose = 1, shuffle = True, epochs = 25):\n",
        "  training_history = model_.fit(\n",
        "      input_data, output_data, batch_size = batch_size,\n",
        "      validation_split = validation_split, verbose = verbose,\n",
        "      shuffle = shuffle, epochs = epochs\n",
        "  ).history\n",
        "\n",
        "  return model_, training_history"
      ],
      "metadata": {
        "id": "qAOaDPw9RGf4"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the sets of training data\n",
        "chars, char_indices, train_x, train_y = process_data(data, input_size = 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LC0MyOJTkEs",
        "outputId": "93000b64-3f71-421a-9155-374057eb996a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique characters : 106\n",
            "  Anyone can see they're not the same by ->  \n",
            "nyone can see they're not the same by th -> e\n",
            "ne can see they're not the same by the n -> u\n",
            "can see they're not the same by the numb -> e\n",
            " see they're not the same by the number  -> o\n",
            "input-size : 1007727, output-size : 1007727\n",
            "Dims of input data : (1007727, 40, 106) \n",
            " Dims of output data : (1007727, 106)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create the  model\n",
        "model = create_model(input_size = 40, vocab_size = len(chars))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2JNYmkFbTyJ",
        "outputId": "86946028-73d0-4f59-d397-1da820356970"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_1 (LSTM)               (None, 128)               120320    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 106)               13674     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 133994 (523.41 KB)\n",
            "Trainable params: 133994 (523.41 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "model, history = train_model(model, train_x[0], train_y[0], epochs = 50)"
      ],
      "metadata": {
        "id": "wx2uC3ZNcYbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model on rest of the dataset\n",
        "import time # this is to introduce the delay after each training\n",
        "for i in range(1, 10):\n",
        "  model = train_model(model, train_x[i], train_y[i], epochs = 100)\n",
        "  time.sleep(60) # delay of 60 seconds after each training\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufu7awOudLhC",
        "outputId": "16676bc0-d222-4d95-fc99-dba17b53763e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "748/748 [==============================] - 5s 7ms/step - loss: 1.6483 - accuracy: 0.5312 - val_loss: 1.6619 - val_accuracy: 0.5182\n",
            "Epoch 2/100\n",
            "748/748 [==============================] - 4s 6ms/step - loss: 1.5952 - accuracy: 0.5414 - val_loss: 1.6470 - val_accuracy: 0.5221\n",
            "Epoch 3/100\n",
            "748/748 [==============================] - 5s 7ms/step - loss: 1.5663 - accuracy: 0.5475 - val_loss: 1.6360 - val_accuracy: 0.5255\n",
            "Epoch 4/100\n",
            "748/748 [==============================] - 5s 7ms/step - loss: 1.5449 - accuracy: 0.5520 - val_loss: 1.6327 - val_accuracy: 0.5315\n",
            "Epoch 5/100\n",
            "748/748 [==============================] - 4s 6ms/step - loss: 1.5267 - accuracy: 0.5566 - val_loss: 1.6305 - val_accuracy: 0.5299\n",
            "Epoch 6/100\n",
            "748/748 [==============================] - 5s 6ms/step - loss: 1.5109 - accuracy: 0.5596 - val_loss: 1.6251 - val_accuracy: 0.5344\n",
            "Epoch 7/100\n",
            "748/748 [==============================] - 5s 7ms/step - loss: 1.4967 - accuracy: 0.5631 - val_loss: 1.6182 - val_accuracy: 0.5350\n",
            "Epoch 8/100\n",
            "748/748 [==============================] - 4s 6ms/step - loss: 1.4838 - accuracy: 0.5658 - val_loss: 1.6094 - val_accuracy: 0.5378\n",
            "Epoch 9/100\n",
            "748/748 [==============================] - 5s 6ms/step - loss: 1.4714 - accuracy: 0.5704 - val_loss: 1.6288 - val_accuracy: 0.5291\n",
            "Epoch 10/100\n",
            "748/748 [==============================] - 6s 8ms/step - loss: 1.4602 - accuracy: 0.5714 - val_loss: 1.6141 - val_accuracy: 0.5392\n",
            "Epoch 11/100\n",
            "748/748 [==============================] - 4s 6ms/step - loss: 1.4485 - accuracy: 0.5748 - val_loss: 1.6126 - val_accuracy: 0.5352\n",
            "Epoch 12/100\n",
            "748/748 [==============================] - 4s 6ms/step - loss: 1.4392 - accuracy: 0.5771 - val_loss: 1.6042 - val_accuracy: 0.5378\n",
            "Epoch 13/100\n",
            "748/748 [==============================] - 6s 8ms/step - loss: 1.4292 - accuracy: 0.5807 - val_loss: 1.6004 - val_accuracy: 0.5370\n",
            "Epoch 14/100\n",
            "748/748 [==============================] - 4s 6ms/step - loss: 1.4197 - accuracy: 0.5821 - val_loss: 1.6080 - val_accuracy: 0.5382\n",
            "Epoch 15/100\n",
            "748/748 [==============================] - 4s 6ms/step - loss: 1.4100 - accuracy: 0.5846 - val_loss: 1.6076 - val_accuracy: 0.5414\n",
            "Epoch 16/100\n",
            "748/748 [==============================] - 5s 7ms/step - loss: 1.4147 - accuracy: 0.5834 - val_loss: 1.6078 - val_accuracy: 0.5402\n",
            "Epoch 17/100\n",
            "748/748 [==============================] - 4s 6ms/step - loss: 1.3948 - accuracy: 0.5876 - val_loss: 1.6057 - val_accuracy: 0.5436\n",
            "Epoch 18/100\n",
            "748/748 [==============================] - 5s 6ms/step - loss: 1.3864 - accuracy: 0.5904 - val_loss: 1.6061 - val_accuracy: 0.5408\n",
            "Epoch 19/100\n",
            "748/748 [==============================] - 5s 7ms/step - loss: 1.3805 - accuracy: 0.5928 - val_loss: 1.6112 - val_accuracy: 0.5340\n",
            "Epoch 20/100\n",
            "748/748 [==============================] - 5s 6ms/step - loss: 1.3726 - accuracy: 0.5951 - val_loss: 1.6040 - val_accuracy: 0.5402\n",
            "Epoch 21/100\n",
            "748/748 [==============================] - 5s 7ms/step - loss: 1.3653 - accuracy: 0.5967 - val_loss: 1.6032 - val_accuracy: 0.5442\n",
            "Epoch 22/100\n",
            "748/748 [==============================] - 4s 6ms/step - loss: 1.3609 - accuracy: 0.5971 - val_loss: 1.6065 - val_accuracy: 0.5384\n",
            "Epoch 23/100\n",
            "748/748 [==============================] - 4s 6ms/step - loss: 1.3514 - accuracy: 0.5994 - val_loss: 1.6118 - val_accuracy: 0.5446\n",
            "Epoch 24/100\n",
            "748/748 [==============================] - 5s 7ms/step - loss: 1.3452 - accuracy: 0.6022 - val_loss: 1.6035 - val_accuracy: 0.5451\n",
            "Epoch 25/100\n",
            "748/748 [==============================] - 4s 6ms/step - loss: 1.3397 - accuracy: 0.6025 - val_loss: 1.6129 - val_accuracy: 0.5444\n",
            "Epoch 26/100\n",
            "748/748 [==============================] - 5s 6ms/step - loss: 1.3336 - accuracy: 0.6040 - val_loss: 1.6092 - val_accuracy: 0.5453\n",
            "Epoch 27/100\n",
            "748/748 [==============================] - 5s 7ms/step - loss: 1.3278 - accuracy: 0.6066 - val_loss: 1.6076 - val_accuracy: 0.5453\n",
            "Epoch 28/100\n",
            "748/748 [==============================] - 4s 6ms/step - loss: 1.3219 - accuracy: 0.6081 - val_loss: 1.6193 - val_accuracy: 0.5444\n",
            "Epoch 29/100\n",
            "748/748 [==============================] - 5s 7ms/step - loss: 1.3154 - accuracy: 0.6098 - val_loss: 1.6050 - val_accuracy: 0.5473\n",
            "Epoch 30/100\n",
            "400/748 [===============>..............] - ETA: 2s - loss: 1.3102 - accuracy: 0.6091"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2EIDAIIvfjtM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}