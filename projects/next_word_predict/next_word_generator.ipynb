{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krahul2024/machine-learning/blob/main/projects/next_word_predict/next_word_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "import tensorflow as tf, numpy as np\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "fpBWBI98kh6f"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset\n",
        "text = '''  Science very efficiently plays the role of being a faithful servant of man. In every walk of life, science is there to serve us. We require the benefits of science whether in our home, in office, in a factory, or outside.Gone are the days when only wealthy people could afford luxuries. Science has made many luxurious items of the past cheaper in price and has brought them within the reach of everybody.Computer technology is one huge benefit of science. Nowadays, it would be unimaginable to consider living without computing technology.A huge number of professions now rely totally on the computer and the internet. Besides, the computer and the internet have become our biggest source of entertainment in our everyday life.A huge number of professions now rely totally on the computer and the internet. Besides, the computer and the internet have become our biggest source of entertainment in our everyday life.Automobiles, an important scientific invention, has made our lives easy by significantly reducing everyday commuting time. The air conditioner is another scientific invention that has made our lives bearable and comfortable in the face of extreme weather conditions. Also, in the field of medical science, high-quality medicines are available that quickly remove any ailment that can happen in everyday life like headache, sprain, cough, allergy, stomach ache, fatigue etc.In spite of its tremendous benefits, there is a negative side to science. Science, unfortunately, has also done some disservice to humanity due to some of its inventions.One of the biggest harms that science has brought to humanity is in the field of armament. Although some hail the invention of gunpowder as a great achievement, humanity must rue the day when this invention happened.Steadily and relentlessly, the use and perfection of gunpowder have taken place in many new and more destructive weapons. As such, humanity now suffers due to weapons like shells, bombs, artillery, and guns. Such weapons threaten the everyday life of all individuals.Another disservice of science has been the emission of pollution. A huge amount of radioactive pollution is emitted in various parts of the world where nuclear energy production happens. Such pollution is very dangerous as it can cause cancer, radioactive sickness, and cardiovascular disease.Of course, who can ignore the massive amount of air pollution caused by automobiles, another scientific invention. Furthermore, automobiles are an everyday part of our lives that emit unimaginable levels of carbon monoxide in the air every year. Consequently, this causes various lung diseases and also contributes to global warming and acid rain.There is no doubt that science has brought about one of the greatest benefits to mankind, in spite of some of its negativities. Furthermore, science certainly has made the most impact in adding comfort to our everyday lives. As such, we must always show utmost respect to scientists for their efforts.The most important or main purpose of science is to explain the facts. Furthermore, there is no restriction in science to explain facts at random. Moreover, science systematizes the facts and builds up theories that give an explanation of such facts.My school is a special place where I spend a big part of my day. The moment I step through the school gates, I feel a sense of excitement and readiness to learn. The classrooms are filled with colorful charts and interesting books that make learning fun. My teacher is kind and helps us understand different subjects.During recess, my friends and I play in the big playground. We have swings, slides, and lots of open space to run around. I love the school library where I can read fascinating stories and discover new things. Our school also organizes events like sports day and annual functions, where we get to showcase our talents and have a great time.My school bag is always packed with notebooks, pencils, and my favorite water bottle. The school bus takes us on a journey filled with laughter and conversations with friends. I am grateful for my school because it not only teaches me academic subjects but also values and good behavior. The teachers are like guides, helping us become better individuals.My best friend is someone very special to me. We do everything together, and she is always there when I need someone to talk to or play with. We have so many fun memories together, from playing at the park to sharing our lunch during school breaks.She is like a sister to me, and we understand each other without even saying a word. We laugh together, cry together, and support each other in everything we do. My best friend has a big heart and is always ready to lend a helping hand.My family is the most important part of my life. I have my mom, dad, and a little brother. We live together, and every day is filled with love and joy. My mom takes care of us and cooks delicious meals. My dad tells us interesting stories and helps with our homework.My little brother is my playmate, and we have so much fun together. We celebrate festivals and special occasions as a family, creating beautiful memories. I feel safe and happy when I am surrounded by my family.The lotus flower is a beautiful and fascinating plant. It grows in ponds and has lovely pink or white petals. What makes the lotus special is that it grows in muddy water but remains pure and beautiful. The petals of the lotus open in the morning and close at night. The lotus is a symbol of purity and is often used in religious ceremonies. Its beauty and resilience teach us important lessons about life. I love watching lotus flowers in ponds, and they always make me feel calm and peaceful. Hello! My name is [Your Name], and I am in Class 2. I am [age] years old and live in [your city]. I have a happy family that includes my parents and siblings. My favorite things to do are [mention your hobbies or activities you enjoy]. I love going to school and learning new things. My best subject is [mention your favorite subject], and I enjoy reading books in my free time. My friends are important to me, and we have lots of fun together.In the future, I want to [mention a dream or goal you have]. I believe in working hard and being kind to others. I am excited about growing up and experiencing new adventures!My favorite food is [mention your favorite food]. It is the most delicious thing I have ever tasted! The aroma of [your favorite food] makes my mouth water, and I can't wait to eat it whenever it's on the table.Whether it's [mention different dishes or varieties], I love every bite. The flavors dance on my taste buds, and I feel happy and satisfied after a good meal of my favorite food. It's like a special treat that I look forward to! Mathematics is my favorite subject in school. I love solving problems and working with numbers. It feels like a puzzle that I enjoy figuring out. My math teacher makes the subject interesting with fun activities and games.I like how math is everywhere in our daily lives, from counting the number of toys I have to measuring ingredients for a recipe. It helps me think logically and improves my problem-solving skills. Learning math is like unlocking a secret code, and I find it exciting!My teacher is like a guiding star in my life. She is kind, patient, and always ready to help us learn. Every day, she comes to class with a smile, making learning enjoyable. She explains things in a way that is easy to understand.My teacher encourages us to ask questions and be curious about the world around us. She is not just a teacher but also a friend who cares about our well-being. I am grateful for havingsuch a wonderful teacher who inspires me to do my best.My school bag is like a treasure chest filled with everything I need for a day of learning. It has colorful notebooks, pencils, erasers, and my favorite storybook. I carry it with me everywhere, and it feels like a companion on my school journey.In my school bag, I also have my lunchbox with yummy snacks that keep me energized. It's amazing how a simple bag can hold so many things that make my school day complete. I take good care of my school bag because it carries the tools of my education.I am lucky to have wonderful neighbors. They are like an extended family, always ready to lend a helping hand. We share happy moments, celebrate festivals together, and support each other during tough times.My neighbor is a friend I can talk to and play with. We have so much fun together, whether it's in our backyard or at their house. Having good neighbors makes the place we live feel like a warm and welcoming community.My village is a peaceful and beautiful place where nature and people coexist harmoniously. Surrounded by green fields and tall trees, my village is a haven of tranquility. The air is fresh, and the sounds of birds and rustling leaves create a soothing melody.In my village, everyone knows each other, and we celebrate festivals and special occasions together. The simple and close-knit community makes living in the village a unique and joyful experience. I feel connected to the earth and the people in my village, making it a special place I am proud to call home.Spring is my favorite season of the year. It brings a burst of colors and life after the cold winter. The flowers bloom, and the trees are covered in fresh green leaves. The air is filled with the sweet fragrance of blossoms, and butterflies dance in the sunlight.I love going for walks in the park during spring, watching the flowers bloom and feeling the gentle breeze. Spring is a time of renewal and growth, and it fills my heart with happiness. The bright days and mild temperatures make it the perfect season for outdoor activities and enjoying nature's beauty.My garden is a magical place where nature comes to life. It is filled with colorful flowers, green plants, and buzzing bees. I love spending time in my garden, watching the plants grow and bloom.I have a small vegetable patch where I plant tomatoes, carrots, and other delicious vegetables. The sound of birds chirping and the scent of blooming flowers make my garden a peaceful retreat. It's a joy to see butterflies and bees visiting the flowers, creating a lively and vibrant atmosphere.Science, the pursuit of knowledge through systematic observation and experimentation, is a beacon that illuminates the mysteries of the universe. It is a vast and ever-expanding realm that touches every aspect of our lives, from the microscopic intricacies of cells to the cosmic dance of galaxies. This essay endeavors to explore the multifaceted world of science, delving into its origins, evolution, and the profound impact it has had on society.At its core, science is a relentless quest for understanding. It is not merely a collection of facts but a dynamic process that involves curiosity, investigation, and the courage to question the unknown. The roots of science trace back to ancient civilizations where early thinkers pondered the workings of the natural world. From the philosophical inquiries of the Greeks to the empirical observations of Chinese scholars, seeds of scientific inquiry were sown across cultures.The Scientific Revolution of the 16th and 17th centuries marked a pivotal moment in the history of science. Visionaries like Copernicus, Galileo, and Newton shattered age-old perceptions, propelling humanity into an era of empirical scrutiny. The heliocentric model, the laws of motion, and the universal law of gravitation laid the groundwork for a new understanding of the cosmos. Science emerged from the shadows of mysticism, embracing reason and evidence as its guiding principles.The Enlightenment further fueled the flames of scientific inquiry. Thinkers such as Bacon and Descartes championed the scientific method—a systematic approach involving observation, experimentation, and analysis. This method became the cornerstone of scientific investigation, providing a structured pathway to unravel the secrets of the natural world. As scientific knowledge expanded, so did the realization of its transformative power.In the 19th century, the rise of industrialization and technological advancements ushered in an era of unprecedented scientific discovery. Darwin's theory of evolution reshaped our understanding of life's diversity, while Maxwell's equations illuminated the nature of electromagnetism. The periodic table, a testament to Mendeleev's ingenuity, organized the building blocks of matter. As steam engines roared to life and telegraph wires spanned continents, science became an integral force shaping the modern world.The 20th century witnessed an explosion of scientific breakthroughs that transformed society on an unprecedented scale. Einstein's theory of relativity revolutionized our understanding of space and time, leading to technological marvels like GPS. The discovery of DNA's structure by Watson and Crick unveiled the genetic code, laying the foundation for modern genetics and biotechnology. The advent of quantum mechanics opened a realm of possibilities at the subatomic level, challenging our perceptions of reality.Science's impact on medicine has been particularly profound. The development of antibiotics revolutionized healthcare, saving countless lives. Vaccines, a triumph of medical science, eradicated deadly diseases and fortified humanity against pandemics. Advances in imaging technology, from X-rays to MRI, have enabled physicians to peer inside the human body with unprecedented clarity, facilitating early diagnosis and precise treatments.Space exploration stands as a testament to humanity's audacious spirit of inquiry. The race to the moon in the mid-20th century captured the world's imagination, showcasing the power of science to transcend earthly boundaries. Today, rovers explore the Martian surface, telescopes peer into distant galaxies, and satellites monitor Earth's climate. Each discovery expands the frontiers of our knowledge and fuels our aspirations to venture further into the cosmos.The interconnectedness of scientific disciplines is a defining feature of contemporary research. The convergence of biology, chemistry, and physics has given rise to interdisciplinary fields such as biochemistry and biophysics. The marriage of computer science and genetics has birthed the field of bioinformatics, revolutionizing our ability to analyze vast genomic datasets. Collaborations across borders and disciplines have become essential to tackling complex challenges, from climate change to global health crises.Yet, for all its triumphs, science is not without ethical and moral considerations. The same technologies that alleviate suffering can also be wielded for destructive purposes. The ethical implications of genetic engineering, artificial intelligence, and nuclear technologies demand careful reflection and responsible governance. As science pushes the boundaries of what is possible, society grapples with the responsibility to ensure these advances are harnessed for the greater good.Education plays a pivotal role in fostering a scientific mindset. The cultivation of curiosity, critical thinking, and a passion for discovery begins in classrooms around the world. Science education goes beyond memorization of facts; it nurtures the spirit of inquiry and empowers individuals to engage with the world as informed and thoughtful citizens. In an era of information overload, the ability to discern reliable evidence from misinformation becomes a crucial skill, emphasizing the role of science literacy in shaping a well-informed society.The democratization of scientific knowledge in the digital age has brought both opportunities and challenges. The internet serves as a vast repository of information, enabling access to scientific literature, educational resources, and collaborative platforms. However, the proliferation of pseudoscience and misinformation also poses a threat, requiring vigilant efforts to promote scientific literacy and critical thinking.As we stand on the threshold of the future, the trajectory of science appears boundless. From unlocking the secrets of the human brain to harnessing the potential of renewable energy, the challenges and possibilities are immense. The pursuit of scientific knowledge is not a solitary endeavor but a collective journey that transcends borders and cultures. In the grand tapestry of human history, science is the thread that weaves together the narrative of exploration, discovery, and understanding.In conclusion, science is a dynamic and transformative force that has shaped the course of human history. From ancient philosophical musings to modern-day technological marvels, the journey of science is marked by curiosity, inquiry, and the unquenchable thirst for knowledge. As we navigate the complexities of the 21st century, the torch of scientific inquiry continues to illuminate the path ahead, guiding humanity toward a future where the wonders of the universe are unveiled, one discovery at a time.In the intricate tapestry of life on Earth, animals stand as vibrant threads, weaving together the story of biodiversity and adaptation. From the microscopic realms of single-celled organisms to the majestic presence of apex predators, the animal kingdom encompasses an astounding array of species, each playing a unique role in the intricate dance of ecosystems. This essay embarks on a journey to explore the multifaceted world of animals, their evolutionary marvels, ecological significance, and the delicate balance that sustains life on our planet.The story of animals unfolds over millions of years, a tale of evolutionary ingenuity and adaptation. The first multicellular animals emerged in Earth's oceans over 600 million years ago, paving the way for the breathtaking diversity we witness today. From the earliest arthropods to the complex vertebrates that grace land, sea, and air, evolution has sculpted a staggering variety of forms, each finely tuned to its environment.The process of natural selection has driven the development of specialized adaptations, enabling animals to thrive in diverse habitats. From the camouflage of chameleons to the swift wings of hummingbirds, these adaptations reflect the ingenious responses of animals to the challenges posed by their surroundings. Evolutionary arms races between predators and prey have led to the development of extraordinary traits, from the speed of cheetahs to the stealth of predators like the mantis shrimp.Animals play pivotal roles in maintaining the delicate balance of ecosystems. They serve as pollinators, seed dispersers, and decomposers, contributing to the functioning of ecosystems that support all life. The intricate web of relationships between animals and their environments highlights the interdependence that characterizes the natural world.Predator-prey interactions regulate populations and prevent unchecked growth that could destabilize ecosystems. The migrations of wildebeests in Africa, the pollination services of bees, and the intricate dance of predator and prey in marine ecosystems exemplify the interconnectedness of life. The loss of key species can have cascading effects, impacting the stability and resilience of entire ecosystems—a phenomenon known as trophic cascades.Biodiversity, the variety of life on Earth, is a testament to the resilience and adaptability of animals. Rainforests, coral reefs, and grasslands harbor a wealth of species, each contributing to the intricate mosaic of life. The loss of biodiversity, driven by factors such as habitat destruction, climate change, and overexploitation, poses a threat not only to individual species but to the functioning of ecosystems and the services they provide.Conservation efforts strive to protect endangered species, restore habitats, and mitigate the impacts of human activities on wildlife. Zoos, wildlife reserves, and protected areas serve as havens for species on the brink of extinction, providing a chance for recovery and eventual reintroduction into the wild. Global initiatives aim to address the root causes of biodiversity loss and foster a harmonious coexistence between humans and the animal kingdom.Beyond their ecological roles, animals exhibit remarkable cognitive abilities and complex social structures. From the problem-solving skills of corvids to the intricate communication of dolphins, these traits challenge traditional notions of animal intelligence. Social animals, such as elephants and wolves, engage in cooperative behaviors, forming familial bonds and intricate social hierarchies.The study of animal behavior sheds light on the rich tapestry of emotions and social dynamics that shape their lives. Elephants mourn their deceased companions, wolves work collaboratively to hunt, and dolphins engage in playful activities that foster social bonds. The recognition of sentience in animals underscores the ethical considerations surrounding their treatment and welfare.The relationship between humans and animals extends beyond the wild realms. Domestication, a process spanning millennia, has led to the emergence of domesticated animals that play integral roles in human societies. Dogs, the first domesticated species, have evolved from working companions to beloved family members, showcasing the transformative power of this interspecies bond.Animals offer companionship, emotional support, and even therapeutic benefits to humans. The therapeutic use of horses in equine-assisted therapy, the comforting presence of therapy dogs, and the role of animals in mitigating stress highlight the multifaceted connections that can exist between humans and animals.Despite the marvels of the animal kingdom, it faces unprecedented challenges in the Anthropocene—the current epoch marked by human influence. Habitat destruction, pollution, climate change, and poaching pose existential threats to many species. The ethical considerations surrounding animal welfare, conservation, and the use of animals in research prompt reflection on humanity's responsibilities as stewards of the planet.Striking a balance between the needs of human societies and the well-being of animals requires thoughtful consideration and sustainable practices. Ethical farming, wildlife conservation, and responsible tourism are avenues through which society can contribute to the preservation of the animal kingdom.In conclusion, animals embody the diversity, resilience, and interconnectedness of life on Earth. Their evolutionary journeys, ecological roles, and complex behaviors enrich the fabric of our planet. As stewards of this shared home,humans bear the responsibility to protect and cherish the animal kingdom, recognizing its intrinsic value and the integral role it plays in sustaining the web of life. Through a harmonious coexistence, we can ensure that future generations inherit a world where the wondrous tapestry of animals continues to thrive.Travelling, the act of journeying from one place to another, is a timeless human endeavor that satisfies our innate curiosity and thirst for discovery. Whether by foot, car, train, or plane, the desire to explore new horizons and experience different cultures is a universal aspect of the human experience. This essay embarks on a simple exploration of travelling, highlighting its joys, challenges, and the profound impact it has on individuals and societies.At its core, travelling is about venturing beyond the familiar, stepping into the unknown, and embracing the richness of diversity. It could be a leisurely stroll through a nearby park, a road trip to a neighboring town, or an international adventure across continents. The essence of travelling lies not only in the physical distance covered but in the mental and emotional landscapes traversed.One of the most enriching aspects of travelling is the opportunity to encounter new cultures. From savoring local cuisine to immersing oneself in traditional festivities, every journey unfolds a tapestry of customs and traditions. The act of breaking bread with strangers, learning a few phrases in a foreign language, and appreciating local customs fosters a sense of connection that transcends geographical boundaries.Travelling often leads us to the embrace of nature's wonders. From serene landscapes to breathtaking vistas, nature becomes a companion on the journey. Whether it's the rustle of leaves in a forest, the rhythmic crashing of waves on a beach, or the panoramic views from a mountain summit, the natural world provides a backdrop of beauty and tranquility that rejuvenates the spirit.While travelling brings joy and discovery, it also comes with its share of challenges. From navigating unfamiliar terrain to facing language barriers, each obstacle becomes an opportunity for growth and adaptation. The resilience developed through overcoming challenges becomes a lasting souvenir of the journey.Travelling opens the door to forging human connections. Whether it's sharing stories with fellow travelers, seeking guidance from locals, or forming bonds with hosts, the threads of human connection weave a narrative of shared experiences. These connections, often fleeting yet profound, remind us of the interconnectedness that unites humanity.Every journey leaves behind a trail of memories and souvenirs. A photograph capturing a sunset, a seashell collected from a distant shore, or the taste of a unique dish—all become tangible reminders of the experiences lived. These mementos serve as windows into the past, allowing one to relive the journey in the quiet moments of reflection.Travelling is not merely about external exploration but also an inward journey of self-discovery. The solitude of a mountain hike, the contemplation by a flowing river, or the quiet moments in a bustling city—all provide opportunities for introspection and self-awareness. Each journey becomes a chapter in the ongoing narrative of personal growth.On a broader scale, travelling contributes to the cultural exchange between societies. The influx of diverse perspectives fosters tolerance and understanding, breaking down stereotypes and prejudices. The tourism industry becomes a bridge that connects people and economies, fostering mutual appreciation and cooperation.As the journey unfolds, there is a unique joy in returning home. The familiar sights, the comfort of one's own bed, and the embrace of loved ones create a sense of grounding. The experiences gained while travelling become a part of the tapestry of one's identity, enriching the understanding of oneself and the world.In conclusion, travelling is a celebration of the human spirit's quest for exploration and connection. Whether a small excursion or a grand adventure, each journey contributes to the collective story of humanity. Through the simple act of travelling, individuals and societies continue to weave a tapestry of experiences, creating a mosaic that reflects the beauty of diversity and the shared journey of life. '''\n",
        "\n",
        "len(text)"
      ],
      "metadata": {
        "id": "rdxRnxfSl5iY",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f05e75a-65ba-4123-ebf5-f0d426e001ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27020"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "\n",
        "words = tokenizer.word_index\n",
        "vocab_size = len(words) + 1\n",
        "len(words), vocab_size"
      ],
      "metadata": {
        "id": "xHxun66dmIio",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03c75499-2f14-4490-bf18-f8de6bab56fa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1464, 1465)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "split_pattern = r'[.,?:;\"()]'\n",
        "sequences = re.split(split_pattern, text)\n",
        "sequences = [seq for seq in sequences if seq] # remove duplicates\n",
        "len(sequences)"
      ],
      "metadata": {
        "id": "wOfWGugQqESa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eef4ba02-c524-436f-844f-547e7c9ad407"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "524"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the sentences into tokens\n",
        "input_seq = []\n",
        "\n",
        "for seq in sequences :\n",
        "  tokens = tokenizer.texts_to_sequences([seq])[0]\n",
        "  for i in range(1, len(tokens)):\n",
        "    input_seq.append(tokens[:i+1])\n",
        "\n",
        "len(input_seq), input_seq[0]"
      ],
      "metadata": {
        "id": "jfkVK5HuMIGV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7ca58b0-27b4-4d1a-a115-bc8f38dceb56"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3727, [11, 175])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max([len(seq) for seq in input_seq])  # get maximumn length sequence\n",
        "print(max_len)\n",
        "\n",
        "# pad the input_sequence with necessary values as to make all the inputs of constant size\n",
        "input_seq = np.array(pad_sequences(input_seq,  maxlen = max_len, padding = 'pre'))\n",
        "input_seq.shape, input_seq[0]"
      ],
      "metadata": {
        "id": "nun1PN9YNG6t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df1fb8ee-51a1-4534-82dd-a53dd1e8c1c4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3727, 22),\n",
              " array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,  11, 175], dtype=int32))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x, y= input_seq[:, :-1], input_seq[:,-1]\n",
        "y = to_categorical(y, num_classes = vocab_size)\n",
        "x.shape, y.shape, y[0]"
      ],
      "metadata": {
        "id": "gF7Ji_3kOM5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7d90587-e4f2-415b-ced7-2f461ffe0907"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3727, 21), (3727, 1465), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create the model\n",
        "model = Sequential([\n",
        "    Embedding(\n",
        "        vocab_size, 100, input_length = x.shape[1]\n",
        "    ),\n",
        "    LSTM(100),\n",
        "    Dense(\n",
        "        vocab_size, activation = 'softmax'\n",
        "    )\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss = 'categorical_crossentropy',\n",
        "    optimizer = 'adam',\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "250w_1hEPCTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x, y, epochs = 150)"
      ],
      "metadata": {
        "id": "WKWlo37IQIDZ",
        "outputId": "c1147176-81f0-46d3-ef4f-7899f9c4abee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.5182 - accuracy: 0.8787\n",
            "Epoch 2/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.5113 - accuracy: 0.8755\n",
            "Epoch 3/150\n",
            "117/117 [==============================] - 5s 44ms/step - loss: 0.5020 - accuracy: 0.8803\n",
            "Epoch 4/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.4938 - accuracy: 0.8809\n",
            "Epoch 5/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.4872 - accuracy: 0.8793\n",
            "Epoch 6/150\n",
            "117/117 [==============================] - 3s 27ms/step - loss: 0.4806 - accuracy: 0.8787\n",
            "Epoch 7/150\n",
            "117/117 [==============================] - 5s 44ms/step - loss: 0.4730 - accuracy: 0.8830\n",
            "Epoch 8/150\n",
            "117/117 [==============================] - 3s 27ms/step - loss: 0.4657 - accuracy: 0.8825\n",
            "Epoch 9/150\n",
            "117/117 [==============================] - 4s 31ms/step - loss: 0.4600 - accuracy: 0.8841\n",
            "Epoch 10/150\n",
            "117/117 [==============================] - 5s 46ms/step - loss: 0.4534 - accuracy: 0.8844\n",
            "Epoch 11/150\n",
            "117/117 [==============================] - 4s 38ms/step - loss: 0.4470 - accuracy: 0.8854\n",
            "Epoch 12/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.4408 - accuracy: 0.8870\n",
            "Epoch 13/150\n",
            "117/117 [==============================] - 3s 27ms/step - loss: 0.4364 - accuracy: 0.8865\n",
            "Epoch 14/150\n",
            "117/117 [==============================] - 4s 32ms/step - loss: 0.4313 - accuracy: 0.8836\n",
            "Epoch 15/150\n",
            "117/117 [==============================] - 5s 40ms/step - loss: 0.4259 - accuracy: 0.8857\n",
            "Epoch 16/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.4206 - accuracy: 0.8878\n",
            "Epoch 17/150\n",
            "117/117 [==============================] - 3s 27ms/step - loss: 0.4157 - accuracy: 0.8903\n",
            "Epoch 18/150\n",
            "117/117 [==============================] - 4s 32ms/step - loss: 0.4128 - accuracy: 0.8884\n",
            "Epoch 19/150\n",
            "117/117 [==============================] - 5s 39ms/step - loss: 0.4082 - accuracy: 0.8873\n",
            "Epoch 20/150\n",
            "117/117 [==============================] - 3s 27ms/step - loss: 0.4036 - accuracy: 0.8900\n",
            "Epoch 21/150\n",
            "117/117 [==============================] - 3s 27ms/step - loss: 0.4006 - accuracy: 0.8911\n",
            "Epoch 22/150\n",
            "117/117 [==============================] - 4s 30ms/step - loss: 0.3964 - accuracy: 0.8924\n",
            "Epoch 23/150\n",
            "117/117 [==============================] - 5s 40ms/step - loss: 0.3935 - accuracy: 0.8919\n",
            "Epoch 24/150\n",
            "117/117 [==============================] - 3s 27ms/step - loss: 0.3899 - accuracy: 0.8916\n",
            "Epoch 25/150\n",
            "117/117 [==============================] - 3s 27ms/step - loss: 0.3872 - accuracy: 0.8908\n",
            "Epoch 26/150\n",
            "117/117 [==============================] - 3s 29ms/step - loss: 0.3834 - accuracy: 0.8889\n",
            "Epoch 27/150\n",
            "117/117 [==============================] - 5s 41ms/step - loss: 0.3790 - accuracy: 0.8940\n",
            "Epoch 28/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3778 - accuracy: 0.8916\n",
            "Epoch 29/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3739 - accuracy: 0.8913\n",
            "Epoch 30/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3707 - accuracy: 0.8913\n",
            "Epoch 31/150\n",
            "117/117 [==============================] - 5s 43ms/step - loss: 0.3687 - accuracy: 0.8916\n",
            "Epoch 32/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3663 - accuracy: 0.8929\n",
            "Epoch 33/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3636 - accuracy: 0.8924\n",
            "Epoch 34/150\n",
            "117/117 [==============================] - 4s 30ms/step - loss: 0.3616 - accuracy: 0.8921\n",
            "Epoch 35/150\n",
            "117/117 [==============================] - 5s 43ms/step - loss: 0.3603 - accuracy: 0.8929\n",
            "Epoch 36/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3568 - accuracy: 0.8916\n",
            "Epoch 37/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3560 - accuracy: 0.8895\n",
            "Epoch 38/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3546 - accuracy: 0.8911\n",
            "Epoch 39/150\n",
            "117/117 [==============================] - 5s 44ms/step - loss: 0.3539 - accuracy: 0.8935\n",
            "Epoch 40/150\n",
            "117/117 [==============================] - 3s 27ms/step - loss: 0.3501 - accuracy: 0.8927\n",
            "Epoch 41/150\n",
            "117/117 [==============================] - 3s 27ms/step - loss: 0.3478 - accuracy: 0.8916\n",
            "Epoch 42/150\n",
            "117/117 [==============================] - 3s 29ms/step - loss: 0.3473 - accuracy: 0.8932\n",
            "Epoch 43/150\n",
            "117/117 [==============================] - 5s 43ms/step - loss: 0.3453 - accuracy: 0.8924\n",
            "Epoch 44/150\n",
            "117/117 [==============================] - 3s 27ms/step - loss: 0.3442 - accuracy: 0.8929\n",
            "Epoch 45/150\n",
            "117/117 [==============================] - 3s 27ms/step - loss: 0.3421 - accuracy: 0.8935\n",
            "Epoch 46/150\n",
            "117/117 [==============================] - 3s 27ms/step - loss: 0.3408 - accuracy: 0.8921\n",
            "Epoch 47/150\n",
            "117/117 [==============================] - 5s 43ms/step - loss: 0.3400 - accuracy: 0.8943\n",
            "Epoch 48/150\n",
            "117/117 [==============================] - 3s 27ms/step - loss: 0.3415 - accuracy: 0.8916\n",
            "Epoch 49/150\n",
            "117/117 [==============================] - 3s 27ms/step - loss: 0.3407 - accuracy: 0.8929\n",
            "Epoch 50/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3376 - accuracy: 0.8913\n",
            "Epoch 51/150\n",
            "117/117 [==============================] - 5s 42ms/step - loss: 0.3353 - accuracy: 0.8937\n",
            "Epoch 52/150\n",
            "117/117 [==============================] - 3s 29ms/step - loss: 0.3340 - accuracy: 0.8935\n",
            "Epoch 53/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3345 - accuracy: 0.8919\n",
            "Epoch 54/150\n",
            "117/117 [==============================] - 3s 27ms/step - loss: 0.3326 - accuracy: 0.8932\n",
            "Epoch 55/150\n",
            "117/117 [==============================] - 5s 42ms/step - loss: 0.3317 - accuracy: 0.8927\n",
            "Epoch 56/150\n",
            "117/117 [==============================] - 3s 29ms/step - loss: 0.3306 - accuracy: 0.8913\n",
            "Epoch 57/150\n",
            "117/117 [==============================] - 3s 27ms/step - loss: 0.3295 - accuracy: 0.8935\n",
            "Epoch 58/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3280 - accuracy: 0.8932\n",
            "Epoch 59/150\n",
            "117/117 [==============================] - 5s 41ms/step - loss: 0.3286 - accuracy: 0.8929\n",
            "Epoch 60/150\n",
            "117/117 [==============================] - 4s 30ms/step - loss: 0.3285 - accuracy: 0.8929\n",
            "Epoch 61/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3279 - accuracy: 0.8908\n",
            "Epoch 62/150\n",
            "117/117 [==============================] - 3s 27ms/step - loss: 0.3266 - accuracy: 0.8935\n",
            "Epoch 63/150\n",
            "117/117 [==============================] - 5s 41ms/step - loss: 0.3252 - accuracy: 0.8932\n",
            "Epoch 64/150\n",
            "117/117 [==============================] - 4s 31ms/step - loss: 0.3277 - accuracy: 0.8908\n",
            "Epoch 65/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3251 - accuracy: 0.8916\n",
            "Epoch 66/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3241 - accuracy: 0.8946\n",
            "Epoch 67/150\n",
            "117/117 [==============================] - 5s 41ms/step - loss: 0.3234 - accuracy: 0.8932\n",
            "Epoch 68/150\n",
            "117/117 [==============================] - 4s 30ms/step - loss: 0.3229 - accuracy: 0.8929\n",
            "Epoch 69/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3225 - accuracy: 0.8940\n",
            "Epoch 70/150\n",
            "117/117 [==============================] - 3s 27ms/step - loss: 0.3208 - accuracy: 0.8908\n",
            "Epoch 71/150\n",
            "117/117 [==============================] - 5s 40ms/step - loss: 0.3211 - accuracy: 0.8913\n",
            "Epoch 72/150\n",
            "117/117 [==============================] - 4s 31ms/step - loss: 0.3211 - accuracy: 0.8913\n",
            "Epoch 73/150\n",
            "117/117 [==============================] - 3s 27ms/step - loss: 0.3208 - accuracy: 0.8927\n",
            "Epoch 74/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3208 - accuracy: 0.8903\n",
            "Epoch 75/150\n",
            "117/117 [==============================] - 4s 38ms/step - loss: 0.3193 - accuracy: 0.8940\n",
            "Epoch 76/150\n",
            "117/117 [==============================] - 4s 33ms/step - loss: 0.3190 - accuracy: 0.8927\n",
            "Epoch 77/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3185 - accuracy: 0.8911\n",
            "Epoch 78/150\n",
            "117/117 [==============================] - 3s 27ms/step - loss: 0.3189 - accuracy: 0.8908\n",
            "Epoch 79/150\n",
            "117/117 [==============================] - 4s 38ms/step - loss: 0.3180 - accuracy: 0.8937\n",
            "Epoch 80/150\n",
            "117/117 [==============================] - 4s 33ms/step - loss: 0.3186 - accuracy: 0.8892\n",
            "Epoch 81/150\n",
            "117/117 [==============================] - 3s 27ms/step - loss: 0.3178 - accuracy: 0.8927\n",
            "Epoch 82/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3180 - accuracy: 0.8935\n",
            "Epoch 83/150\n",
            "117/117 [==============================] - 4s 38ms/step - loss: 0.3173 - accuracy: 0.8897\n",
            "Epoch 84/150\n",
            "117/117 [==============================] - 4s 34ms/step - loss: 0.3178 - accuracy: 0.8935\n",
            "Epoch 85/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3161 - accuracy: 0.8900\n",
            "Epoch 86/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3161 - accuracy: 0.8932\n",
            "Epoch 87/150\n",
            "117/117 [==============================] - 4s 37ms/step - loss: 0.3160 - accuracy: 0.8927\n",
            "Epoch 88/150\n",
            "117/117 [==============================] - 4s 35ms/step - loss: 0.3154 - accuracy: 0.8940\n",
            "Epoch 89/150\n",
            "117/117 [==============================] - 3s 27ms/step - loss: 0.3149 - accuracy: 0.8924\n",
            "Epoch 90/150\n",
            "117/117 [==============================] - 3s 27ms/step - loss: 0.3145 - accuracy: 0.8943\n",
            "Epoch 91/150\n",
            "117/117 [==============================] - 4s 35ms/step - loss: 0.3141 - accuracy: 0.8927\n",
            "Epoch 92/150\n",
            "117/117 [==============================] - 4s 36ms/step - loss: 0.3143 - accuracy: 0.8937\n",
            "Epoch 93/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3137 - accuracy: 0.8932\n",
            "Epoch 94/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3137 - accuracy: 0.8919\n",
            "Epoch 95/150\n",
            "117/117 [==============================] - 4s 35ms/step - loss: 0.3140 - accuracy: 0.8935\n",
            "Epoch 96/150\n",
            "117/117 [==============================] - 4s 36ms/step - loss: 0.3134 - accuracy: 0.8919\n",
            "Epoch 97/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3137 - accuracy: 0.8927\n",
            "Epoch 98/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3130 - accuracy: 0.8916\n",
            "Epoch 99/150\n",
            "117/117 [==============================] - 4s 36ms/step - loss: 0.3138 - accuracy: 0.8900\n",
            "Epoch 100/150\n",
            "117/117 [==============================] - 4s 36ms/step - loss: 0.3136 - accuracy: 0.8935\n",
            "Epoch 101/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3136 - accuracy: 0.8913\n",
            "Epoch 102/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3128 - accuracy: 0.8932\n",
            "Epoch 103/150\n",
            "117/117 [==============================] - 4s 34ms/step - loss: 0.3124 - accuracy: 0.8927\n",
            "Epoch 104/150\n",
            "117/117 [==============================] - 4s 37ms/step - loss: 0.3116 - accuracy: 0.8932\n",
            "Epoch 105/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3127 - accuracy: 0.8946\n",
            "Epoch 106/150\n",
            "117/117 [==============================] - 3s 27ms/step - loss: 0.3119 - accuracy: 0.8897\n",
            "Epoch 107/150\n",
            "117/117 [==============================] - 4s 34ms/step - loss: 0.3112 - accuracy: 0.8935\n",
            "Epoch 108/150\n",
            "117/117 [==============================] - 4s 38ms/step - loss: 0.3112 - accuracy: 0.8929\n",
            "Epoch 109/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3115 - accuracy: 0.8940\n",
            "Epoch 110/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3113 - accuracy: 0.8927\n",
            "Epoch 111/150\n",
            "117/117 [==============================] - 4s 35ms/step - loss: 0.3107 - accuracy: 0.8911\n",
            "Epoch 112/150\n",
            "117/117 [==============================] - 4s 38ms/step - loss: 0.3116 - accuracy: 0.8927\n",
            "Epoch 113/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3121 - accuracy: 0.8935\n",
            "Epoch 114/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3189 - accuracy: 0.8905\n",
            "Epoch 115/150\n",
            "117/117 [==============================] - 4s 34ms/step - loss: 0.3242 - accuracy: 0.8908\n",
            "Epoch 116/150\n",
            "117/117 [==============================] - 4s 38ms/step - loss: 0.3172 - accuracy: 0.8921\n",
            "Epoch 117/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3149 - accuracy: 0.8911\n",
            "Epoch 118/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3113 - accuracy: 0.8921\n",
            "Epoch 119/150\n",
            "117/117 [==============================] - 4s 33ms/step - loss: 0.3099 - accuracy: 0.8940\n",
            "Epoch 120/150\n",
            "117/117 [==============================] - 4s 38ms/step - loss: 0.3101 - accuracy: 0.8919\n",
            "Epoch 121/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3097 - accuracy: 0.8919\n",
            "Epoch 122/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3099 - accuracy: 0.8948\n",
            "Epoch 123/150\n",
            "117/117 [==============================] - 4s 32ms/step - loss: 0.3098 - accuracy: 0.8935\n",
            "Epoch 124/150\n",
            "117/117 [==============================] - 5s 38ms/step - loss: 0.3094 - accuracy: 0.8948\n",
            "Epoch 125/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3098 - accuracy: 0.8932\n",
            "Epoch 126/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3093 - accuracy: 0.8937\n",
            "Epoch 127/150\n",
            "117/117 [==============================] - 4s 32ms/step - loss: 0.3095 - accuracy: 0.8932\n",
            "Epoch 128/150\n",
            "117/117 [==============================] - 5s 40ms/step - loss: 0.3088 - accuracy: 0.8935\n",
            "Epoch 129/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3093 - accuracy: 0.8921\n",
            "Epoch 130/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3092 - accuracy: 0.8919\n",
            "Epoch 131/150\n",
            "117/117 [==============================] - 4s 31ms/step - loss: 0.3087 - accuracy: 0.8929\n",
            "Epoch 132/150\n",
            "117/117 [==============================] - 5s 40ms/step - loss: 0.3090 - accuracy: 0.8940\n",
            "Epoch 133/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3095 - accuracy: 0.8908\n",
            "Epoch 134/150\n",
            "117/117 [==============================] - 3s 27ms/step - loss: 0.3091 - accuracy: 0.8927\n",
            "Epoch 135/150\n",
            "117/117 [==============================] - 4s 31ms/step - loss: 0.3090 - accuracy: 0.8897\n",
            "Epoch 136/150\n",
            "117/117 [==============================] - 5s 42ms/step - loss: 0.3092 - accuracy: 0.8916\n",
            "Epoch 137/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3091 - accuracy: 0.8962\n",
            "Epoch 138/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3090 - accuracy: 0.8937\n",
            "Epoch 139/150\n",
            "117/117 [==============================] - 4s 32ms/step - loss: 0.3087 - accuracy: 0.8924\n",
            "Epoch 140/150\n",
            "117/117 [==============================] - 5s 40ms/step - loss: 0.3090 - accuracy: 0.8903\n",
            "Epoch 141/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3105 - accuracy: 0.8913\n",
            "Epoch 142/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3085 - accuracy: 0.8937\n",
            "Epoch 143/150\n",
            "117/117 [==============================] - 5s 43ms/step - loss: 0.3086 - accuracy: 0.8943\n",
            "Epoch 144/150\n",
            "117/117 [==============================] - 5s 46ms/step - loss: 0.3086 - accuracy: 0.8913\n",
            "Epoch 145/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3089 - accuracy: 0.8900\n",
            "Epoch 146/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3081 - accuracy: 0.8913\n",
            "Epoch 147/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3081 - accuracy: 0.8954\n",
            "Epoch 148/150\n",
            "117/117 [==============================] - 5s 44ms/step - loss: 0.3082 - accuracy: 0.8924\n",
            "Epoch 149/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3077 - accuracy: 0.8921\n",
            "Epoch 150/150\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.3078 - accuracy: 0.8927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('word_predict.keras')"
      ],
      "metadata": {
        "id": "0VOJXp52QTnK"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_word(sequence):\n",
        "    tokens = tokenizer.texts_to_sequences([sequence])[0]\n",
        "    input_sequence = np.array(pad_sequences([tokens], maxlen = max_len-1, padding = 'pre'))\n",
        "    pred = np.argmax(model.predict(input_sequence, verbose = 0))\n",
        "    # print(pred)\n",
        "    for key in words :\n",
        "        if(words[key] == pred) :\n",
        "            return key\n",
        "\n",
        "predict_word('what ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8y82z2vlT1pJ",
        "outputId": "5aa67be5-93cf-4c15-8136-ab7cc14bc270"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'makes'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test Cases\n",
        "test_cases = [\n",
        "    \"I\",\n",
        "    \"The\",\n",
        "    \"She\",\n",
        "    \"My\",\n",
        "    \"Reading\",\n",
        "    \"Tomorrow\",\n",
        "    \"Coffee\",\n",
        "    \"Music\",\n",
        "    \"In\",\n",
        "    \"I\",\n",
        "    \"My\",\n",
        "    \"Pizza\",\n",
        "    \"After\",\n",
        "    \"The\",\n",
        "    \"Nature\",\n",
        "    \"I\",\n",
        "    \"Every\",\n",
        "    \"Learning\",\n",
        "    \"When\",\n",
        "    \"My\",\n",
        "    \"To\",\n",
        "    \"On\",\n",
        "    \"Creativity\",\n",
        "    \"The\",\n",
        "    \"The\",\n",
        "    \"I\",\n",
        "    \"The\",\n",
        "    \"My\",\n",
        "    \"A\",\n",
        "    \"I am\",\n",
        "    \"The sun\",\n",
        "    \"She always\",\n",
        "    \"My favorite\",\n",
        "    \"Reading a\",\n",
        "    \"Tomorrow, I\",\n",
        "    \"Coffee tastes\",\n",
        "    \"Music helps\",\n",
        "    \"In the\",\n",
        "    \"I enjoy\",\n",
        "    \"My pet\",\n",
        "    \"Pizza topped\",\n",
        "    \"After work,\",\n",
        "    \"The secret\",\n",
        "    \"Nature is\",\n",
        "    \"I feel\",\n",
        "    \"Every morning,\",\n",
        "    \"Learning a new\",\n",
        "    \"When it\",\n",
        "    \"My favorite\",\n",
        "    \"To stay\",\n",
        "    \"On weekends,\",\n",
        "    \"Creativity is\",\n",
        "    \"The key to\",\n",
        "    \"The more\",\n",
        "    \"I believe\",\n",
        "    \"The ocean\",\n",
        "    \"My favorite\",\n",
        "    \"A smile can\",\n",
        "]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ytuPg0tFUVl0"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for test in test_cases :\n",
        "    print(test , '--->', predict_word(test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMWdMeSNYc0A",
        "outputId": "7f97cb8c-98f0-4659-9313-1ce926e34330"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I ---> love\n",
            "The ---> air\n",
            "She ---> is\n",
            "My ---> best\n",
            "Reading ---> plants\n",
            "Tomorrow ---> a\n",
            "Coffee ---> a\n",
            "Music ---> a\n",
            "In ---> the\n",
            "I ---> love\n",
            "My ---> best\n",
            "Pizza ---> a\n",
            "After ---> a\n",
            "The ---> air\n",
            "Nature ---> becomes\n",
            "I ---> love\n",
            "Every ---> journey\n",
            "Learning ---> a\n",
            "When ---> we\n",
            "My ---> best\n",
            "To ---> we\n",
            "On ---> a\n",
            "Creativity ---> a\n",
            "The ---> air\n",
            "The ---> air\n",
            "I ---> love\n",
            "The ---> air\n",
            "My ---> best\n",
            "A ---> huge\n",
            "I am ---> grateful\n",
            "The sun ---> air\n",
            "She always ---> makes\n",
            "My favorite ---> things\n",
            "Reading a ---> great\n",
            "Tomorrow, I ---> love\n",
            "Coffee tastes ---> a\n",
            "Music helps ---> us\n",
            "In the ---> grand\n",
            "I enjoy ---> reading\n",
            "My pet ---> best\n",
            "Pizza topped ---> a\n",
            "After work, ---> as\n",
            "The secret ---> code\n",
            "Nature is ---> a\n",
            "I feel ---> connected\n",
            "Every morning, ---> and\n",
            "Learning a new ---> things\n",
            "When it ---> grows\n",
            "My favorite ---> things\n",
            "To stay ---> we\n",
            "On weekends, ---> a\n",
            "Creativity is ---> a\n",
            "The key to ---> the\n",
            "The more ---> of\n",
            "I believe ---> in\n",
            "The ocean ---> air\n",
            "My favorite ---> things\n",
            "A smile can ---> illuminates\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model details\n",
        "model = tf.keras.models.load_model('./word_predict.keras')\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "weights = model.get_weights()\n",
        "for layer_num, layer_weights in enumerate(weights):\n",
        "    print(f\"Layer {layer_num + 1} weights:\")\n",
        "    print(layer_weights)\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdJ4kNiYYlRj",
        "outputId": "207e815f-5649-481b-8270-baa92f25b325"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 21, 100)           146500    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 100)               80400     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1465)              147965    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 374865 (1.43 MB)\n",
            "Trainable params: 374865 (1.43 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Layer 1 weights:\n",
            "[[-0.17542201  0.12056589 -0.29568052 ...  0.35867316 -0.21984422\n",
            "  -0.45314258]\n",
            " [-0.91813564 -1.1242932  -0.30737665 ... -0.331754    0.40253046\n",
            "   0.05973782]\n",
            " [-0.39671904  0.11428074  0.2352433  ...  0.08937423  0.47003818\n",
            "   0.04626284]\n",
            " ...\n",
            " [-0.53813124  0.13492288  0.3498005  ...  0.19404538 -0.13106732\n",
            "  -0.18516397]\n",
            " [ 0.29470348 -0.291238    0.56255215 ...  0.14244619 -0.09775602\n",
            "  -0.3452676 ]\n",
            " [-0.24963196 -0.03804117  0.17377411 ...  0.04607135  0.3287296\n",
            "  -0.39329186]]\n",
            "\n",
            "\n",
            "Layer 2 weights:\n",
            "[[ 0.27596438 -0.01963698  0.03069501 ...  1.4502424   0.99489045\n",
            "   0.69577974]\n",
            " [ 0.4108217   0.07512055 -0.0152314  ...  0.6787163   0.8437953\n",
            "  -0.9939949 ]\n",
            " [-0.10783483 -0.04099751 -0.05358231 ... -0.6889981   0.09187933\n",
            "  -0.04754633]\n",
            " ...\n",
            " [ 0.13204162  0.12936325 -0.15717705 ... -1.8509907   1.218845\n",
            "  -0.14176863]\n",
            " [-0.15039976  0.02818183  0.1583324  ...  0.16294217  0.28194207\n",
            "  -0.11137526]\n",
            " [-0.2016129  -0.27654123 -0.03147387 ... -0.7314359  -1.415699\n",
            "  -0.80838776]]\n",
            "\n",
            "\n",
            "Layer 3 weights:\n",
            "[[-0.14367631  0.13289289  0.1358167  ... -0.23678035  0.34192103\n",
            "  -0.09603423]\n",
            " [ 0.23312333  0.09439977  0.06156548 ... -0.14241171 -0.51225644\n",
            "   0.5289463 ]\n",
            " [ 0.30072266  0.06020484 -0.03562134 ...  0.2688341  -0.51888424\n",
            "   0.54180825]\n",
            " ...\n",
            " [ 0.08604795 -0.10689789 -0.21124445 ... -0.11417896  0.6112907\n",
            "   0.19315325]\n",
            " [ 0.09379806  0.07089131  0.08887769 ...  0.19592091 -0.08604572\n",
            "  -0.03548072]\n",
            " [-0.13487332  0.03738599  0.05439738 ...  1.8123044  -0.4391704\n",
            "   0.1875808 ]]\n",
            "\n",
            "\n",
            "Layer 4 weights:\n",
            "[ 2.91257113e-01  6.29035532e-02 -2.44456772e-02  1.11683279e-01\n",
            "  8.33806992e-02  3.51122677e-01  4.61489499e-01  2.86796484e-02\n",
            "  1.03945332e-02  3.17774825e-02  2.17929687e-02  6.44924790e-02\n",
            "  2.41470695e-01  2.52192974e-01  1.20960111e-02 -7.48307258e-02\n",
            "  7.30898678e-01  5.39296214e-03  5.36898058e-03  8.35475251e-02\n",
            "  1.19647048e-01  6.23221219e-01  2.20894888e-01  2.13389993e-01\n",
            " -9.29859281e-02 -2.49586031e-02  7.96071216e-02  6.57891273e-01\n",
            " -4.14337441e-02 -1.66532840e-03 -7.92169478e-03 -3.91257182e-03\n",
            "  9.30163711e-02  3.36048484e-01  7.87151903e-02  1.06203474e-01\n",
            "  1.39563948e-01  6.78679764e-01  8.58754143e-02  7.40233660e-01\n",
            "  1.57849537e-03  2.56386369e-01  9.65628847e-02  2.35614076e-01\n",
            "  4.15631942e-02 -1.61138326e-01  4.23815027e-02 -5.98043352e-02\n",
            "  7.13964641e-01 -1.46618355e-02  2.17570271e-02  2.16926150e-02\n",
            "  1.75231189e-01  4.90139239e-02 -1.98217742e-02  1.82989687e-01\n",
            "  8.49512398e-01  2.36348156e-02  5.42954309e-03 -7.00869411e-02\n",
            "  1.98033899e-01  5.76286495e-01  7.74047971e-02 -3.60184871e-02\n",
            "  6.06724083e-01  5.07716089e-02  1.51157454e-02  3.70361835e-01\n",
            "  9.75976437e-02  5.51604867e-01 -1.54583072e-02 -1.84728652e-02\n",
            "  4.80490446e-01  7.89634809e-02  1.07811533e-01  3.72491367e-02\n",
            "  2.95615988e-03  2.96240505e-02 -5.67834824e-02 -3.67836021e-02\n",
            "  9.07145798e-01  2.12060213e-02  5.03328443e-02  2.62692645e-02\n",
            "  1.19141303e-01  8.35322663e-02  2.30423138e-01  8.36330354e-02\n",
            "  4.91026312e-01  1.07317246e-01 -5.23501486e-02  3.59648436e-01\n",
            "  1.09011054e-01  1.09357223e-01  1.19673878e-01  6.96599960e-01\n",
            "  1.60940200e-01  1.27566576e-01  1.11308850e-01  3.46164219e-02\n",
            "  9.85811889e-01  1.08029592e+00  1.02455950e+00  1.01400614e+00\n",
            "  1.08135867e+00  9.25906062e-01  1.12057388e+00  1.04304612e+00\n",
            "  1.02985430e+00  1.07490611e+00  1.04480541e+00  1.08843315e+00\n",
            "  9.29418981e-01  9.15220261e-01  1.04951370e+00  1.06315577e+00\n",
            "  1.05876744e+00  1.07985198e+00  9.57538068e-01  9.42857385e-01\n",
            "  1.02542746e+00  1.08887196e+00  9.44340229e-01  1.00012600e+00\n",
            "  1.02438295e+00  9.96200979e-01  1.07934272e+00  7.77828515e-01\n",
            "  9.72202659e-01  1.06468868e+00  1.02108681e+00  1.03004396e+00\n",
            "  1.03511226e+00  1.00781786e+00  1.10533845e+00  1.06158042e+00\n",
            "  1.05767787e+00  1.06924903e+00  1.04856062e+00  7.83029437e-01\n",
            "  9.93942618e-01  1.07632577e+00  1.02561629e+00  8.56772304e-01\n",
            "  1.09398770e+00  1.01283598e+00  9.99745548e-01  1.01135242e+00\n",
            "  9.74286139e-01  1.00324738e+00  9.96879578e-01  1.00739300e+00\n",
            "  1.06560004e+00  9.82528269e-01  1.09150624e+00  1.10067868e+00\n",
            "  1.24278843e+00  1.04469848e+00  1.03912330e+00  1.00353515e+00\n",
            "  9.28344846e-01  1.03340340e+00  1.08504140e+00  9.48046148e-01\n",
            "  1.18846595e+00  1.07189202e+00  9.33900118e-01  9.29611683e-01\n",
            "  1.02230418e+00  1.06730020e+00  1.02691031e+00  1.06621003e+00\n",
            "  1.37500274e+00  1.00603163e+00  1.13294017e+00  1.03599524e+00\n",
            "  1.10380709e+00  1.04630053e+00  1.02436328e+00  1.03486085e+00\n",
            "  8.64453375e-01  1.08057916e+00  1.02234590e+00  1.03917348e+00\n",
            "  1.07242990e+00  1.11045265e+00  1.00121391e+00  1.08887792e+00\n",
            "  1.20841348e+00  1.10394597e+00  1.05934000e+00  9.79289472e-01\n",
            "  1.02089298e+00  1.02005994e+00  9.37253654e-01  1.08720589e+00\n",
            "  1.10009909e+00  1.07664549e+00  9.74867582e-01  9.95836258e-01\n",
            "  1.34759573e-02  1.51129663e-01  1.17194206e-01  6.52019307e-02\n",
            " -3.67229953e-02 -5.66246584e-02 -6.85084090e-02 -2.79455222e-02\n",
            " -9.25444961e-02 -7.15556219e-02  7.17703402e-02  1.29550546e-01\n",
            "  2.66044820e-03  2.75088777e-03 -4.39644493e-02 -7.18174055e-02\n",
            "  1.64393410e-01  9.43257585e-02 -1.06694929e-01  1.55715421e-02\n",
            "  1.16135515e-02  4.87151444e-02 -1.75393417e-01  5.77069297e-02\n",
            "  7.71969883e-03 -9.68856215e-02  8.81232321e-02  2.39845607e-02\n",
            " -7.89860636e-02  6.54154420e-02 -4.11858559e-02 -5.28908446e-02\n",
            "  9.73570347e-02 -1.90030355e-02 -6.22452088e-02  1.03639886e-02\n",
            "  9.94321704e-02  8.01376328e-02 -9.76120774e-03  1.02020502e-01\n",
            "  5.11400849e-02  7.06527208e-04  5.72098009e-02  3.89785878e-02\n",
            "  1.18384853e-01  8.58527049e-02 -4.68971170e-02 -7.77184516e-02\n",
            " -1.00692399e-02 -2.45074462e-02  2.95853168e-02  1.72173826e-03\n",
            "  8.28987062e-02 -6.38558716e-02 -1.44730508e-02  9.38213319e-02\n",
            " -7.13506434e-03 -5.12480028e-02  7.52467662e-02  7.80640021e-02\n",
            " -7.68885612e-02 -1.17428452e-02 -7.85294101e-02  5.38897030e-02\n",
            " -1.82557255e-01  1.83604937e-02 -3.93740721e-02  2.98972949e-02\n",
            " -9.90843121e-03  4.37815189e-02  8.48181099e-02 -8.50566030e-02\n",
            "  1.03415810e-01 -4.81571630e-03 -7.34479278e-02  1.41269550e-01\n",
            " -1.38991326e-03 -1.51540572e-02  5.08220792e-02 -5.87279052e-02\n",
            " -2.30789818e-02  2.85625048e-02  8.89131129e-02  1.58432610e-02\n",
            "  1.39847010e-01  1.26995251e-01  1.65059026e-02 -8.88644978e-02\n",
            " -1.40082166e-01  8.61885324e-02 -7.66111389e-02  2.62347683e-02\n",
            "  1.60522889e-02 -1.85286254e-02  1.24425404e-02 -2.22844690e-01\n",
            " -5.95093593e-02  1.59521177e-02  6.74591139e-02  5.45889102e-02\n",
            "  2.24858984e-01  4.11069393e-01  3.70980680e-01  2.52665609e-01\n",
            "  2.49118537e-01  4.29949403e-01  6.11699075e-02 -2.31251165e-01\n",
            " -6.51095584e-02 -2.56673187e-01  1.34460613e-01 -3.47115807e-02\n",
            " -2.73280233e-01 -2.87314862e-01  8.89475644e-01  1.78962469e-01\n",
            "  8.51480544e-01  5.85174263e-02  2.00880602e-01 -1.06776804e-01\n",
            "  3.04182589e-01  3.64944059e-03  3.91370088e-01  1.31671410e-02\n",
            "  2.25707546e-01  3.99007648e-01 -2.29955297e-02  7.01139390e-01\n",
            "  9.40129280e-01  2.04890128e-02  3.80919248e-01  5.09224057e-01\n",
            "  3.21367115e-01  3.10981542e-01 -1.38536736e-01  2.80710042e-01\n",
            "  4.32944417e-01  6.38853610e-01  3.82961668e-02  2.12758571e-01\n",
            " -1.81806937e-01  1.08022153e-01  1.30185977e-01  4.97422606e-01\n",
            "  1.31434962e-01  1.35651380e-01  5.82747340e-01 -3.21423352e-01\n",
            "  1.84122309e-01  2.83804119e-01  5.18520713e-01  2.69821316e-01\n",
            " -1.74057826e-01  1.84360027e-01  9.87583473e-02 -2.25495636e-01\n",
            "  4.47018981e-01  7.69882798e-02  4.87153292e-01 -2.12956011e-01\n",
            "  1.63654581e-01  2.57364154e-01  8.67252052e-02  4.55656290e-01\n",
            "  5.05318761e-01  4.27556753e-01  4.50456560e-01 -1.24156073e-01\n",
            "  2.25528300e-01  6.97350144e-01  1.84282869e-01  9.51594952e-03\n",
            "  7.66799390e-01  1.80011511e-01 -2.67772049e-01  2.46879950e-01\n",
            " -7.71394819e-02  6.04292393e-01  8.45711589e-01 -2.36422360e-01\n",
            "  5.30700743e-01  5.73047698e-01  4.98683512e-01  4.10388380e-01\n",
            " -6.45781227e-04 -3.35905887e-02  3.56948465e-01 -2.62613654e-01\n",
            "  2.19014615e-01 -3.06439042e-01  5.91555461e-02 -1.07958071e-01\n",
            "  2.77074367e-01 -8.05981904e-02 -2.44887948e-01  1.05726194e+00\n",
            " -1.67088091e-01  3.01327407e-02 -2.35212594e-01 -3.17213893e-01]\n",
            "\n",
            "\n",
            "Layer 5 weights:\n",
            "[[-0.3390687   0.1589443  -0.28836945 ... -0.40457612  0.16155136\n",
            "  -1.1667658 ]\n",
            " [-0.22682731 -0.08767904  0.70567    ...  0.458073   -0.15348063\n",
            "  -1.6588441 ]\n",
            " [-0.45039847 -0.1581284  -0.8209152  ... -0.77299094 -1.1154402\n",
            "   0.21965541]\n",
            " ...\n",
            " [ 0.42360616 -0.45504323 -0.12955648 ... -0.4121204   0.13003491\n",
            "   0.29738462]\n",
            " [-0.23879784 -1.0465227   0.32716313 ... -0.90404046  0.11105306\n",
            "  -0.07438157]\n",
            " [-0.41783613  0.00827147 -0.08112891 ... -0.69843054 -2.0401921\n",
            "   0.14371954]]\n",
            "\n",
            "\n",
            "Layer 6 weights:\n",
            "[-0.90570116 -0.33427176 -0.8103304  ... -0.0429631  -0.5933425\n",
            " -0.12794864]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xQwnPxSxeBpx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}