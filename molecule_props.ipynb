{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVh5xmxATGF+MjVAFVAuSb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krahul2024/machine-learning/blob/main/molecule_props.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7tVEQOhUx-rK"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy\n",
        "import pandas as pd\n",
        "import matplotlib.pylab as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as Fun\n",
        "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU\n",
        "\n",
        "import sys\n",
        "\n",
        "from torch_geometric.datasets import QM9\n",
        "from torch_geometric.nn import GCNConv, GINConv\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
        "\n",
        "# specify the local data path\n",
        "HERE = Path(_dh[-1])\n",
        "DATA = HERE / \"data\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "qm9 = QM9(root=DATA)\n",
        "qm9[0]\n",
        "\n",
        "# get one regression target\n",
        "y_target = pd.DataFrame(qm9.data.y.numpy())\n",
        "qm9.data.y = torch.Tensor(y_target[0])\n",
        "\n",
        "qm9 = qm9.shuffle()\n",
        "\n",
        "# data split\n",
        "data_size = 30000\n",
        "train_index = int(data_size * 0.8)\n",
        "test_index = train_index + int(data_size * 0.1)\n",
        "val_index = test_index + int(data_size * 0.1)\n",
        "\n",
        "\n",
        "# normalizing the data\n",
        "data_mean = qm9.data.y[0:train_index].mean()\n",
        "data_std = qm9.data.y[0:train_index].std()\n",
        "\n",
        "qm9.data.y = (qm9.data.y - data_mean) / data_std\n",
        "\n",
        "# datasets into DataLoader\n",
        "train_loader = DataLoader(qm9[0:train_index], batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(qm9[train_index:test_index], batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(qm9[test_index:val_index], batch_size=64, shuffle=True)\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    \"\"\"Graph Convolutional Network class with 3 convolutional layers and a linear layer\"\"\"\n",
        "\n",
        "    def __init__(self, dim_h):\n",
        "        \"\"\"init method for GCN\n",
        "\n",
        "        Args:\n",
        "            dim_h (int): the dimension of hidden layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(qm9.num_features, dim_h)\n",
        "        self.conv2 = GCNConv(dim_h, dim_h)\n",
        "        self.conv3 = GCNConv(dim_h, dim_h)\n",
        "        self.lin = torch.nn.Linear(dim_h, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        e = data.edge_index\n",
        "        x = data.x\n",
        "\n",
        "        x = self.conv1(x, e)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, e)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, e)\n",
        "        x = global_mean_pool(x, data.batch)\n",
        "\n",
        "        x = Fun.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class GIN(torch.nn.Module):\n",
        "    \"\"\"Graph Isomorphism Network class with 3 GINConv layers and 2 linear layers\"\"\"\n",
        "\n",
        "    def __init__(self, dim_h):\n",
        "        \"\"\"Initializing GIN class\n",
        "\n",
        "        Args:\n",
        "            dim_h (int): the dimension of hidden layers\n",
        "        \"\"\"\n",
        "        super(GIN, self).__init__()\n",
        "        self.conv1 = GINConv(\n",
        "            Sequential(Linear(11, dim_h), BatchNorm1d(dim_h), ReLU(), Linear(dim_h, dim_h), ReLU())\n",
        "        )\n",
        "        self.conv2 = GINConv(\n",
        "            Sequential(\n",
        "                Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(), Linear(dim_h, dim_h), ReLU()\n",
        "            )\n",
        "        )\n",
        "        self.conv3 = GINConv(\n",
        "            Sequential(\n",
        "                Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(), Linear(dim_h, dim_h), ReLU()\n",
        "            )\n",
        "        )\n",
        "        self.lin1 = Linear(dim_h, dim_h)\n",
        "        self.lin2 = Linear(dim_h, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x = data.x\n",
        "        edge_index = data.edge_index\n",
        "        batch = data.batch\n",
        "\n",
        "        # Node embeddings\n",
        "        h = self.conv1(x, edge_index)\n",
        "        h = h.relu()\n",
        "        h = self.conv2(h, edge_index)\n",
        "        h = h.relu()\n",
        "        h = self.conv3(h, edge_index)\n",
        "\n",
        "        # Graph-level readout\n",
        "        h = global_add_pool(h, batch)\n",
        "\n",
        "        h = self.lin1(h)\n",
        "        h = h.relu()\n",
        "        h = Fun.dropout(h, p=0.5, training=self.training)\n",
        "        h = self.lin2(h)\n",
        "\n",
        "        return h\n",
        "\n",
        "def training(loader, model, loss, optimizer):\n",
        "    \"\"\"Training one epoch\n",
        "\n",
        "    Args:\n",
        "        loader (DataLoader): loader (DataLoader): training data divided into batches\n",
        "        model (nn.Module): GNN model to train on\n",
        "        loss (nn.functional): loss function to use during training\n",
        "        optimizer (torch.optim): optimizer during training\n",
        "\n",
        "    Returns:\n",
        "        float: training loss\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    current_loss = 0\n",
        "    for d in loader:\n",
        "        optimizer.zero_grad()\n",
        "        d.x = d.x.float()\n",
        "\n",
        "        out = model(d)\n",
        "\n",
        "        l = loss(out, torch.reshape(d.y, (len(d.y), 1)))\n",
        "        current_loss += l / len(loader)\n",
        "        l.backward()\n",
        "        optimizer.step()\n",
        "    return current_loss, model\n",
        "\n",
        "\n",
        "def validation(loader, model, loss):\n",
        "    \"\"\"Validation\n",
        "\n",
        "    Args:\n",
        "        loader (DataLoader): validation set in batches\n",
        "        model (nn.Module): current trained model\n",
        "        loss (nn.functional): loss function\n",
        "\n",
        "    Returns:\n",
        "        float: validation loss\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    for d in loader:\n",
        "        out = model(d)\n",
        "        l = loss(out, torch.reshape(d.y, (len(d.y), 1)))\n",
        "        val_loss += l / len(loader)\n",
        "    return val_loss\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def testing(loader, model):\n",
        "    \"\"\"Testing\n",
        "\n",
        "    Args:\n",
        "        loader (DataLoader): test dataset\n",
        "        model (nn.Module): trained model\n",
        "\n",
        "    Returns:\n",
        "        float: test loss\n",
        "    \"\"\"\n",
        "    loss = torch.nn.MSELoss()\n",
        "    test_loss = 0\n",
        "    test_target = numpy.empty((0))\n",
        "    test_y_target = numpy.empty((0))\n",
        "    for d in loader:\n",
        "        out = model(d)\n",
        "        # NOTE\n",
        "        # out = out.view(d.y.size())\n",
        "        l = loss(out, torch.reshape(d.y, (len(d.y), 1)))\n",
        "        test_loss += l / len(loader)\n",
        "\n",
        "        # save prediction vs ground truth values for plotting\n",
        "        test_target = numpy.concatenate((test_target, out.detach().numpy()[:, 0]))\n",
        "        test_y_target = numpy.concatenate((test_y_target, d.y.detach().numpy()))\n",
        "\n",
        "    return test_loss, test_target, test_y_target\n",
        "\n",
        "\n",
        "\n",
        "def train_epochs(epochs, model, train_loader, val_loader, path):\n",
        "    \"\"\"Training over all epochs\n",
        "\n",
        "    Args:\n",
        "        epochs (int): number of epochs to train for\n",
        "        model (nn.Module): the current model\n",
        "        train_loader (DataLoader): training data in batches\n",
        "        val_loader (DataLoader): validation data in batches\n",
        "        path (string): path to save the best model\n",
        "\n",
        "    Returns:\n",
        "        array: returning train and validation losses over all epochs, prediction and ground truth values for training data in the last epoch\n",
        "    \"\"\"\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
        "    loss = torch.nn.MSELoss()\n",
        "\n",
        "    train_target = numpy.empty((0))\n",
        "    train_y_target = numpy.empty((0))\n",
        "    train_loss = numpy.empty(epochs)\n",
        "    val_loss = numpy.empty(epochs)\n",
        "    best_loss = math.inf\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss, model = training(train_loader, model, loss, optimizer)\n",
        "        v_loss = validation(val_loader, model, loss)\n",
        "        if v_loss < best_loss:\n",
        "            torch.save(model.state_dict(), path)\n",
        "        for d in train_loader:\n",
        "            out = model(d)\n",
        "            if epoch == epochs - 1:\n",
        "                # record truly vs predicted values for training data from last epoch\n",
        "                train_target = numpy.concatenate((train_target, out.detach().numpy()[:, 0]))\n",
        "                train_y_target = numpy.concatenate((train_y_target, d.y.detach().numpy()))\n",
        "\n",
        "        train_loss[epoch] = epoch_loss.detach().numpy()\n",
        "        val_loss[epoch] = v_loss.detach().numpy()\n",
        "\n",
        "        # print current train and val loss\n",
        "        if epoch % 2 == 0:\n",
        "            print(\n",
        "                \"Epoch: \"\n",
        "                + str(epoch)\n",
        "                + \", Train loss: \"\n",
        "                + str(epoch_loss.item())\n",
        "                + \", Val loss: \"\n",
        "                + str(v_loss.item())\n",
        "            )\n",
        "    return train_loss, val_loss, train_target, train_y_target\n",
        "\n",
        "\n",
        "# training GCN for 10 epochs\n",
        "epochs = 10\n",
        "\n",
        "model = GCN(dim_h=128)\n",
        "\n",
        "# Remember to change the path if you want to keep the previously trained model\n",
        "gcn_train_loss, gcn_val_loss, gcn_train_target, gcn_train_y_target = train_epochs(\n",
        "    epochs, model, train_loader, test_loader, \"GCN_model.pt\"\n",
        ")\n",
        "\n",
        "# Training GIN for 10 epochs\n",
        "model = GIN(dim_h=64)\n",
        "\n",
        "# Remember to change the path if you want to keep the previously trained model\n",
        "gin_train_loss, gin_val_loss, gin_train_target, gin_train_y_target = train_epochs(\n",
        "    epochs, model, train_loader, test_loader, \"GIN_model.pt\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbFT3BIGyz_g",
        "outputId": "d9a84397-2c33-4dd4-807b-ba15790accc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Train loss: 0.8948440551757812, Val loss: 0.8403787612915039\n",
            "Epoch: 2, Train loss: 0.8345817923545837, Val loss: 0.8170331716537476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the dataset has many regression targets, we will only focus on one of the tasks, which is the prediction of the dipole moment . For this tutorial, we only sample a subset of QM9. This keeps the runtime low and this is still enough to show some first results. The dataset is split into training, validation and test sets with a  split ratio. In addition, we normalize the training data () and apply the same mean and standard deviation to the test and validation set."
      ],
      "metadata": {
        "id": "7ZJ589nw1b3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss(gcn_train_loss, gcn_val_loss, gin_train_loss, gin_val_loss):\n",
        "    \"\"\"Plot the loss for each epoch\n",
        "\n",
        "    Args:\n",
        "        epochs (int): number of epochs\n",
        "        train_loss (array): training losses for each epoch\n",
        "        val_loss (array): validation losses for each epoch\n",
        "    \"\"\"\n",
        "    plt.plot(gcn_train_loss, label=\"Train loss (GCN)\")\n",
        "    plt.plot(gcn_val_loss, label=\"Val loss (GCN)\")\n",
        "    plt.plot(gin_train_loss, label=\"Train loss (GIN)\")\n",
        "    plt.plot(gin_val_loss, label=\"Val loss (GIN)\")\n",
        "    plt.legend()\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.title(\"Model Loss\")\n",
        "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_targets(pred, ground_truth):\n",
        "    \"\"\"Plot true vs predicted value in a scatter plot\n",
        "\n",
        "    Args:\n",
        "        pred (array): predicted values\n",
        "        ground_truth (array): ground truth values\n",
        "    \"\"\"\n",
        "    f, ax = plt.subplots(figsize=(6, 6))\n",
        "    ax.scatter(pred, ground_truth, s=0.5)\n",
        "    plt.xlim(-2, 7)\n",
        "    plt.ylim(-2, 7)\n",
        "    ax.axline((1, 1), slope=1)\n",
        "    plt.xlabel(\"Predicted Value\")\n",
        "    plt.ylabel(\"Ground truth\")\n",
        "    plt.title(\"Ground truth vs prediction\")\n",
        "    plt.show()\n",
        "\n",
        "# Plot overall losses of GIN and GCN\n",
        "\n",
        "plot_loss(gcn_train_loss, gcn_val_loss, gin_train_loss, gin_val_loss)"
      ],
      "metadata": {
        "id": "YBNxXZZP0QK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot target and prediction for training data\n",
        "\n",
        "plot_targets(gin_train_target, gin_train_y_target)"
      ],
      "metadata": {
        "id": "EMy4zIGQMu1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate test loss from the best GCN model (according to validation loss)\n",
        "\n",
        "# load our model\n",
        "model = GCN(dim_h=128)\n",
        "model.load_state_dict(torch.load(\"GCN_best-model-parameters.pt\"))\n",
        "\n",
        "# calculate test loss\n",
        "gcn_test_loss, gcn_test_target, gcn_test_y = testing(test_loader, model)\n",
        "\n",
        "print(\"Test Loss for GCN: \" + str(gcn_test_loss.item()))\n",
        "\n",
        "# plot prediction vs ground truth\n",
        "plot_targets(gcn_test_target, gcn_test_y)"
      ],
      "metadata": {
        "id": "GywZ1gw9MxD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate test loss from the best GIN model (according to validation loss)\n",
        "\n",
        "# load our model\n",
        "model = GIN(dim_h=64)\n",
        "model.load_state_dict(torch.load(\"GIN_best-model-parameters.pt\"))\n",
        "\n",
        "# calculate test loss\n",
        "gin_test_loss, gin_test_target, gin_test_y = testing(test_loader, model)\n",
        "\n",
        "print(\"Test Loss for GIN: \" + str(gin_test_loss.item()))\n",
        "\n",
        "# plot prediction vs ground truth\n",
        "plot_targets(gin_test_target, gin_test_y)"
      ],
      "metadata": {
        "id": "OJHo8M-0MyyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q7etZILmM0s5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}